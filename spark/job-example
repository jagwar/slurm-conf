from pyspark.sql import SparkSession
from  pyspark.sql.functions import lit, rand

spark = (
        SparkSession.builder
        .config("spark.submit.deployMode", "client") \
        .config("spark.executor.memory", "16GB")
        .config("spark.executor.memoryOverhead", "8GB")
        .config("spark.task.maxFailures", "100")
        .config("spark.driver.port", "39800")
        .config("spark.driver.blockManager.port", "38000")
        .config("spark.port.maxRetries", "100")
        .master("spark://master:7077")
        .appName("spark-stats")
        .getOrCreate()
    )
    
df = spark.read.parquet("/fsx/guillaume/gpfswork/rech/six/commun/aws-team/rech/six/commun/conda/envs/tr11-176B-ml/lib/python3.8/site-packages/pandas/tests/io/data/parquet/simple.parquet")

df.count()
